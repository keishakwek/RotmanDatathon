{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, MultiTaskElasticNetCV, ElasticNetCV\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from joblib import dump, load\n",
        "import hdbscan\n",
        "import shap\n",
        "import warnings\n",
        "import re\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.simplefilter('ignore')"
      ],
      "metadata": {
        "id": "EH3q61UlZBXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test(ytest, ypred):\n",
        "  '''\n",
        "  runs MSE, R2, RMSE, and MAE\n",
        "  '''\n",
        "  mse = mean_squared_error(ytest, ypred)\n",
        "  r2 = r2_score(ytest, ypred)\n",
        "  rmse = np.sqrt(mse)\n",
        "  mae = mean_absolute_error(ytest, ypred)\n",
        "\n",
        "  return mse, r2, rmse, mae"
      ],
      "metadata": {
        "id": "N_Dve39UZ8jP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN Imputation"
      ],
      "metadata": {
        "id": "uJqVR87YQ0Gf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2L2IzY_26FX"
      },
      "outputs": [],
      "source": [
        "#cleaning the raw data\n",
        "raw_data = pd.read_csv('~/Downloads/sheet1.csv')\n",
        "df = raw_data.dropna(how='all')[:-2].drop(columns=['Country Code','Time Code']).replace('..', np.nan)\n",
        "feature_matrix = df.iloc[:,2:]\n",
        "for col in feature_matrix.columns:\n",
        "    feature_matrix[col] = pd.to_numeric(feature_matrix[col], errors='coerce')\n",
        "cleaned_df = pd.concat([df.iloc[:,0:2], feature_matrix], axis=1).reset_index().iloc[:, 1:]\n",
        "\n",
        "#imputing using knn\n",
        "scaler = StandardScaler()\n",
        "data_scaled_df = pd.DataFrame(scaler.fit_transform(feature_matrix), columns=feature_matrix.columns, index=feature_matrix.index)\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "data_imputed = imputer.fit_transform(data_scaled_df)\n",
        "imputed_df = pd.DataFrame(data_imputed, columns=feature_matrix.columns, index=feature_matrix.index)\n",
        "\n",
        "#inversing the scaling so we keep original information.\n",
        "scaled_imputed_df = pd.DataFrame(scaler.inverse_transform(imputed_df), columns=feature_matrix.columns, index=feature_matrix.index)\n",
        "scaled_imputed_final_df = pd.concat([cleaned_df.iloc[:, 0:2], scaled_imputed_df],axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Merging Data with WRI"
      ],
      "metadata": {
        "id": "zVqYYQWkRJE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/knn_imputed_dataset.csv')\n",
        "wri = pd.read_csv('/content/drive/My Drive/worldriskindex-trend.csv')\n",
        "\n",
        "data = data.rename(columns={'Country Code': 'ISO3.Code'})\n",
        "\n",
        "data['Year'] = data['Time Code'].str.extract(r'(\\d{4})').astype(int)  # Extract year from 'YR2000' and convert to integer\n",
        "\n",
        "wri_selected_columns = wri[['ISO3.Code', 'Year', 'W']]\n",
        "\n",
        "merged_data = data.merge(wri_selected_columns, on=['ISO3.Code', 'Year'], how='left')\n",
        "\n",
        "merged_data=merged_data.drop(columns=['Year'])\n",
        "merged_data=merged_data.rename(columns={'ISO3.Code':'Country Code'})\n",
        "print(\"Shape of the merged dataframe:\", merged_data.shape)\n",
        "print(\"Sample of the merged dataframe:\")\n",
        "print(merged_data.head())\n",
        "\n",
        "merged_data.to_csv('/content/drive/My Drive/merged_data_with_W.csv', index=True)\n"
      ],
      "metadata": {
        "id": "sprMnIpkRNZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Matrix"
      ],
      "metadata": {
        "id": "6tqmmR6WQ3il"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = merged_data\n",
        "correlation_matrix = df.iloc[:,3:].corr()\n",
        "\n",
        "#taking only most correlated values\n",
        "filtered_corr_matrix = correlation_matrix[(correlation_matrix > 0.5) | (correlation_matrix < -0.5)].replace(np.nan, '')"
      ],
      "metadata": {
        "id": "FiO3CBvVQ7y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Principle Components Analysis"
      ],
      "metadata": {
        "id": "2Cp3CR7fQ9zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(0.85)\n",
        "xpca = pca.fit_transform(xtrain)\n",
        "xpca.shape\n",
        "\n",
        "# running pca on linear regression\n",
        "model = LinearRegression()\n",
        "model.fit(xpca, ytrain)\n",
        "model.score(xpca, ytrain)\n",
        "xtest = scaler.fit_transform(xtest)\n",
        "xtest = pd.DataFrame(xtest, columns=x.columns)\n",
        "xpca_test = pca.transform(xtest)\n",
        "ypred = model.predict(xpca_test)\n",
        "results = model_test(ytest, ypred)"
      ],
      "metadata": {
        "id": "ISiV4qCHQ89f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HDBScan"
      ],
      "metadata": {
        "id": "LIWo03Uke-Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"/content/drive/My Drive/merged_data_with_W.csv\"\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "data['Year'] = data['Time Code'].str.extract('(\\d{4})').astype(int)\n",
        "\n",
        "\n",
        "data = data.sort_values(by=[\"Country Code\", \"Year\"])\n",
        "\n",
        "# Select only target numeric columns for clustering\n",
        "numerical_cols = [\n",
        "    \"Human capital index (HCI) (scale 0-1) [HD.HCI.OVRL]\",\n",
        "    \"Current health expenditure per capita, PPP (current international $) [SH.XPD.CHEX.PP.CD]\",\n",
        "    \"GNI per capita, PPP (current international $) [NY.GNP.PCAP.PP.CD]\",\n",
        "    \"Machinery and transport equipment (% of value added in manufacturing) [NV.MNF.MTRN.ZS.UN]\",\n",
        "    \"Individuals using the Internet (% of population) [IT.NET.USER.ZS]\",\n",
        "    \"Imports of goods and services (current US$) [NE.IMP.GNFS.CD]\",\n",
        "    \"Electric power consumption (kWh per capita) [EG.USE.ELEC.KH.PC]\",\n",
        "    \"Power outages in firms in a typical month (number) [IC.ELC.OUTG]\",\n",
        "    \"Cost of business start-up procedures (% of GNI per capita) [IC.REG.COST.PC.ZS]\",\n",
        "    \"Logistics performance index: Overall (1=low to 5=high) [LP.LPI.OVRL.XQ]\"\n",
        "]\n",
        "\n",
        "data_numeric = data[numerical_cols].dropna()  # Drop rows with missing values\n",
        "\n",
        "# Normalize the data for clustering\n",
        "scaler = StandardScaler()\n",
        "data_scaled = scaler.fit_transform(data_numeric)\n",
        "\n",
        "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric='euclidean')\n",
        "cluster_labels = clusterer.fit_predict(data_scaled)\n",
        "\n",
        "data['Cluster'] = cluster_labels\n",
        "\n",
        "data.to_csv(\"/content/drive/My Drive/clustered_data.csv\", index=False)\n",
        "print(\"Clustered data saved to clustered_data.csv\")\n",
        "\n",
        "# PCA for 2D visualization\n",
        "pca = PCA(n_components=2)\n",
        "pca_result = pca.fit_transform(data_scaled)\n",
        "data['PCA1'] = pca_result[:, 0]\n",
        "data['PCA2'] = pca_result[:, 1]\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.scatterplot(\n",
        "    x='PCA1', y='PCA2', hue='Cluster', palette='tab10', data=data, legend='full'\n",
        ")\n",
        "plt.title(\"HDBSCAN Clustering (PCA Projection)\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(title=\"Cluster\", loc=\"best\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Cluster-wise LPI Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(\n",
        "    x='Cluster',\n",
        "    y=\"Logistics performance index: Overall (1=low to 5=high) [LP.LPI.OVRL.XQ]\",\n",
        "    data=data\n",
        ")\n",
        "plt.title(\"LPI Distribution by Cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Logistics Performance Index\")\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "outliers = data[data['Cluster'] == -1]\n",
        "\n",
        "outlier_info = outliers[['Country Code', 'Year']]\n",
        "\n",
        "# Display the outlier information\n",
        "print(\"Outlier Countries and Years:\")\n",
        "print(outlier_info)\n",
        "\n",
        "data['Cluster'] = cluster_labels\n",
        "\n",
        "cluster_0_data = data[data['Cluster'] == 0]\n",
        "\n",
        "cluster_0_countries = cluster_0_data[['Country Code', 'Year']]\n",
        "\n",
        "# Display the names and years of countries in cluster \"1\"\n",
        "print(cluster_0_countries)\n"
      ],
      "metadata": {
        "id": "6k1k-RjKe-8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelling a Multivariate Elastic Net Regression"
      ],
      "metadata": {
        "id": "KrDRcAZ2ambW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = scaled_imputed_final_df\n",
        "lpi_colsnames = [col for col in data if 'Logistics' in col]\n",
        "lpi_colsnames.append('Air transport, freight (million ton-km) [IS.AIR.GOOD.MT.K1]')\n",
        "\n",
        "y = data[[lpi_colsnames]]\n",
        "x = data.iloc[:,1:].drop(lpi_colsnames, axis=1)\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y1, test_size=0.2, random_state=100)\n",
        "\n",
        "xtrain_scaled = scaler.fit_transform(xtrain)\n",
        "ytrain_scaled = scaler.fit_transform(ytrain)\n",
        "\n",
        "xtest_scaled = scaler.fit_transform(xtest)\n",
        "ytest_scaled = scaler.fit_transform(ytest)\n",
        "\n",
        "multitaskelasticnet = MultiTaskElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=5, random_state=42)\n",
        "multitaskelasticnet.fit(xtrain_scaled, ytrain_scaled)\n",
        "\n",
        "ypred = multitaskelasticnet.predict(xtest_scaled)\n",
        "results = test_model(ytest, ypred)"
      ],
      "metadata": {
        "id": "UuXTw9DTahyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the model with all the data we have"
      ],
      "metadata": {
        "id": "FPVDAYhVbrqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_scaled = scaler.fit_transform(x)\n",
        "y_scaled = scaler.fit_transform(y)\n",
        "multitaskelasticnet_fin = MultiTaskElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1], cv=5, random_state=100)\n",
        "multitaskelasticnet_fin.fit(x_scaled, y_scaled)"
      ],
      "metadata": {
        "id": "mU1N8YzzbbiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performing a SHAP analysis to see how the model predicts data"
      ],
      "metadata": {
        "id": "MJ_uI01Db1fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "explainer = shap.Explainer(multitaskelasticnet_fin.predict, x_scaled)\n",
        "shap_values = explainer(x_scaled, max_evals=600)\n",
        "list_of_features = x.columns.to_list()\n",
        "shap.summary_plot(shap_values[:,:,0], features=x, feature_names=list_of_features)"
      ],
      "metadata": {
        "id": "4rH6j9zXb0fi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shap.waterfall_plot(\n",
        "    shap.Explanation(values=shap_values[0, :, 0],\n",
        "                     data=x.iloc[0],\n",
        "                     feature_names=list_of_features)\n",
        ")"
      ],
      "metadata": {
        "id": "kB4VxmSxcKSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forecasting Model"
      ],
      "metadata": {
        "id": "ODoXlYEOcNin"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using linear regression to predict each country's economic metrics for the next five years"
      ],
      "metadata": {
        "id": "aILXptokcR13"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = data[lpi_colsnames]\n",
        "x = data.iloc[:,1:].drop(lpi_colsnames, axis=1)\n",
        "\n",
        "feats = data.drop(lpi_colsnames, axis=1)\n",
        "\n",
        "for year in range(2024,2030):\n",
        "    new_rows_list = []\n",
        "    for country in feats['Country Code'].unique():\n",
        "        new_rows = {'Country Code': country,'Time': year}\n",
        "        for feature in feature_names:\n",
        "            temp = feats[feats['Country Code'] == country][['Country Code', 'Time', feature]]\n",
        "            xval = temp[['Time']]\n",
        "            yval = temp[[feature]]\n",
        "            model = LinearRegression()\n",
        "            model.fit(xval, yval)\n",
        "            prediction = model.predict([[year]])\n",
        "            pred_feature = {feature: prediction[0]}\n",
        "            new_rows.update(pred_feature)\n",
        "        new_rows_list.append(new_rows)\n",
        "\n",
        "    new_year_preds = pd.DataFrame(new_rows_list)\n",
        "    new_year_preds = new_year_preds.applymap(lambda x: x[0] if isinstance(x, (list, np.ndarray)) else x)\n",
        "    feats = pd.concat([feats, new_year_preds], axis=0)\n"
      ],
      "metadata": {
        "id": "CDYp5OQDcQiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the new feature matrix with predicted values for 2024-2029 and the fitted Elastic Net model to predict the outcome of LPI and Freight."
      ],
      "metadata": {
        "id": "AO6Eb-hgcmjt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ypred = multitaskelasticnet_fin.predict(feats.iloc[:,1:])\n",
        "pred_colnames = y.columns.to_list()\n",
        "ypred_df = pd.DataFrame(ypred, columns = pred_colnames)\n",
        "\n",
        "feats = feats.reset_index(drop=True)\n",
        "ypred_df = ypred_df.reset_index(drop=True)\n",
        "\n",
        "forecasted_data = pd.concat([feats, ypred_df], axis=1)"
      ],
      "metadata": {
        "id": "WadknxwDch5A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}